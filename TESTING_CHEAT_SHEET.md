# ðŸŽ¯ Testing Cheat Sheet
**Quick Reference Guide - ISTQB & OWASP Aligned**

---

## ðŸ¤– **AUTOMATION TESTING** (ISTQB Test Automation Engineer)

### ðŸ—ï¸ **Test Automation Architecture**

| Layer | ðŸŽ¯ Purpose | ðŸ› ï¸ Components |
|-------|-----------|----------------|
| **Test Generation Layer** | Create test cases | Test data, test scripts |
| **Test Definition Layer** | Define test structure | Test frameworks, BDD specs |
| **Test Execution Layer** | Run tests | Test runners, CI/CD |
| **Test Adaptation Layer** | Interface with SUT | Drivers, stubs, mocks |
| **Testability Layer** | Enable automation | Test hooks, APIs |

### ðŸŽ¨ **Test Automation Frameworks**

| Framework Type | ðŸŽ¯ Approach | âœ… Pros | ðŸ“ Example |
|----------------|-------------|---------|------------|
| **Linear Scripting** | Record & playback | Quick start | Selenium IDE |
| **Data-Driven** | Separate data from scripts | Reusable, scalable | CSV/JSON test data |
| **Keyword-Driven** | Keywords = actions | Non-technical friendly | Robot Framework |
| **Modular** | Reusable modules | Maintainable | Page Object Model |
| **Hybrid** | Combines approaches | Best of all | POM + Data-Driven |

### ðŸ”§ **Test Automation Design Patterns**

| Pattern | ðŸŽ¯ Purpose | ðŸ“ Implementation |
|---------|-----------|-------------------|
| **Page Object Model (POM)** | Encapsulate page logic | Separate page classes |
| **Page Factory** | Initialize page objects | @FindBy annotations |
| **Singleton** | Single instance | Driver management |
| **Factory** | Create objects | Test data generation |
| **Builder** | Construct objects | Test scenario building |
| **Strategy** | Algorithm selection | Different test approaches |
| **Observer** | Event handling | Test listeners |

### ðŸ“Š **Test Automation Lifecycle**

```
1ï¸âƒ£ Planning        â†’ Define scope, tools, approach
2ï¸âƒ£ Design          â†’ Architecture, patterns, framework
3ï¸âƒ£ Implementation  â†’ Write scripts, create framework
4ï¸âƒ£ Execution       â†’ Run tests, CI/CD integration
5ï¸âƒ£ Maintenance     â†’ Update scripts, refactor
6ï¸âƒ£ Retirement     â†’ Archive obsolete tests
```

### ðŸ› ï¸ **Test Automation Tools**

| Tool Category | ðŸŽ¯ Purpose | ðŸ› ï¸ Examples |
|---------------|-----------|-------------|
| **Test Execution** | Run tests | pytest, JUnit, TestNG |
| **Test Management** | Organize tests | TestRail, Zephyr |
| **CI/CD Integration** | Automate runs | Jenkins, GitHub Actions |
| **Reporting** | Test results | Allure, pytest-html |
| **Code Coverage** | Measure coverage | pytest-cov, JaCoCo |
| **Mocking/Stubbing** | Isolate components | Mockito, unittest.mock |

### ðŸ”„ **Test Automation Maintenance**

| Activity | ðŸŽ¯ Purpose | âš ï¸ When Needed |
|----------|-----------|----------------|
| **Refactoring** | Improve code quality | Code smells detected |
| **Updating** | Adapt to changes | SUT changes |
| **Debugging** | Fix failures | Tests failing |
| **Optimization** | Improve performance | Slow execution |
| **Version Control** | Track changes | All changes |

### ðŸ“ˆ **Test Automation Metrics**

| Metric | ðŸŽ¯ What It Shows | âœ… Target |
|--------|-------------------|-----------|
| **Automation Coverage** | % tests automated | > 70% |
| **Test Execution Time** | How fast tests run | < 10 min |
| **Pass Rate** | % tests passing | > 95% |
| **Maintenance Effort** | Time to update tests | < 20% of dev time |
| **ROI** | Return on investment | Positive after 3-6 months |
| **Flakiness Rate** | % unstable tests | < 5% |

### âœ… **Test Automation Best Practices**

```
âœ… Start with high-value, stable tests
âœ… Use Page Object Model for maintainability
âœ… Separate test data from test logic
âœ… Implement proper wait strategies
âœ… Use meaningful test names
âœ… Keep tests independent
âœ… Clean up test data
âœ… Use version control
âœ… Document framework decisions
âœ… Regular code reviews
```

### ðŸš€ **Quick Commands**

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=. --cov-report=html

# Run specific test
pytest tests/saucedemo/test_login.py -v

# Run with HTML report
pytest --html=report.html --self-contained-html

# Run with markers
pytest -m smoke

# Run in parallel
pytest -n auto

# Debug test
pytest --pdb tests/saucedemo/test_login.py
```

---

## âš¡ **PERFORMANCE TESTING** (ISTQB Performance)

### ðŸ“Š **Test Types**

| Type | ðŸŽ¯ Purpose | ðŸ“ˆ Load | â±ï¸ Duration | ðŸŽ¯ Goal |
|------|-----------|--------|-------------|---------|
| **Load Testing** | Normal load | Expected users | 30-60 min | Verify performance |
| **Stress Testing** | Beyond capacity | Max + 20% | Until failure | Find breaking point |
| **Spike Testing** | Sudden traffic | 0 â†’ Max â†’ 0 | 5-10 min | Handle spikes |
| **Volume Testing** | Large data | Normal load | 1+ hour | Data handling |
| **Endurance Testing** | Long duration | Normal load | 4+ hours | Memory leaks |

### ðŸ“ˆ **Key Metrics**

| Metric | ðŸŽ¯ What It Means | âœ… Good | âŒ Bad |
|--------|-------------------|---------|--------|
| **Response Time** | Time to respond | < 2s | > 5s |
| **Throughput** | Requests/second | High | Low |
| **Error Rate** | Failed requests | < 1% | > 5% |
| **CPU Usage** | Processor load | < 70% | > 90% |
| **Memory Usage** | RAM consumption | Stable | Growing |
| **P95/P99** | 95th/99th percentile | < 3s | > 10s |

### ðŸ› ï¸ **Tools Quick Reference**

| Tool | ðŸŽ¯ Best For | ðŸ“ Command |
|------|-------------|------------|
| **k6** | Modern, scriptable | `k6 run script.js` |
| **Locust** | Python-based | `locust -f locustfile.py` |
| **JMeter** | GUI + scripting | `jmeter -n -t test.jmx` |
| **Gatling** | Scala, reports | `gatling.sh` |

### âœ… **Performance Checklist**

```
âœ… Define performance goals (SLA)
âœ… Create realistic test data
âœ… Monitor resources (CPU, Memory, Network)
âœ… Run baseline test first
âœ… Compare before/after changes
âœ… Document bottlenecks
âœ… Retest after fixes
```

---

## ðŸ” **PENETRATION TESTING** (OWASP Top 10)

### ðŸŽ¯ **OWASP Top 10:2021**

| # | Vulnerability | ðŸ”´ Risk | ðŸ› ï¸ Test For | âœ… Status |
|---|---------------|---------|-------------|-----------|
| **A01** | Broken Access Control | ðŸ”´ðŸ”´ðŸ”´ High | IDOR, privilege escalation | âœ… Covered |
| **A02** | Cryptographic Failures | ðŸ”´ðŸ”´ðŸ”´ High | Plaintext passwords, weak encryption | âšª Next |
| **A03** | Injection | ðŸ”´ðŸ”´ðŸ”´ High | SQLi, NoSQLi, Command Injection | âœ… Covered |
| **A04** | Insecure Design | ðŸ”´ðŸ”´ Medium | Business logic flaws | âšª Next |
| **A05** | Security Misconfiguration | ðŸ”´ðŸ”´ Medium | Default configs, exposed files | âšª Next |
| **A06** | Vulnerable Components | ðŸ”´ðŸ”´ Medium | Outdated libraries, CVEs | âšª Next |
| **A07** | Auth Failures | ðŸ”´ðŸ”´ðŸ”´ High | Weak passwords, session issues | âœ… Covered |
| **A08** | Data Integrity Failures | ðŸ”´ðŸ”´ Medium | Unsigned updates, CI/CD attacks | âšª Next |
| **A09** | Logging Failures | ðŸ”´ Low | Missing logs, log injection | âšª Next |
| **A10** | SSRF | ðŸ”´ðŸ”´ Medium | Internal network access | âšª Next |

### ðŸ” **Common Attack Vectors**

| Attack | ðŸŽ¯ Target | ðŸ’¥ Impact | ðŸ› ï¸ Tool |
|--------|-----------|-----------|---------|
| **SQL Injection** | Database | Data breach, auth bypass | sqlmap, manual |
| **XSS** | Users | Cookie theft, defacement | Burp Suite, manual |
| **IDOR** | API endpoints | Unauthorized access | Manual testing |
| **CSRF** | State-changing ops | Unauthorized actions | Burp Suite |
| **SSRF** | Internal services | Internal network access | Manual testing |
| **XXE** | XML parsers | File read, SSRF | Manual testing |

### ðŸŽ¯ **Testing Methodology**

```
1ï¸âƒ£ Reconnaissance    â†’ ðŸ” Info gathering
2ï¸âƒ£ Scanning          â†’ ðŸŽ¯ Find vulnerabilities  
3ï¸âƒ£ Exploitation      â†’ ðŸ’¥ Prove impact
4ï¸âƒ£ Post-Exploitation â†’ ðŸ“Š Maintain access
5ï¸âƒ£ Reporting        â†’ ðŸ“ Document findings
```

### âœ… **Quick Commands**

```bash
# SQL Injection test
python tools/attacks/attack_sqli_basic.py

# XSS test
python tools/attacks/attack_xss_basic.py

# API endpoint discovery
python tools/discovery/discovery_api_endpoints.py

# OWASP Top 10 comprehensive test
python tools/testing/test_dvwa_owasp_top10.py
```

---

## ðŸ”— **CROSS-REPO INTEGRATION**

### ðŸ¤–âš¡ **Automation + Performance**

```
âœ… Add performance assertions to UI tests
âœ… Fail test if page load > 3s
âœ… Monitor API response times in automation
âœ… Track performance trends over time
```

### ðŸ¤–ðŸ” **Automation + Security**

```
âœ… Test for XSS in form submissions
âœ… Check for exposed secrets in responses
âœ… Validate authentication flows
âœ… Test authorization boundaries
```

### âš¡ðŸ” **Performance + Security**

```
âœ… Run attacks under load (hybrid testing)
âœ… Test if system fails faster under stress + attack
âœ… Monitor resource usage during attacks
âœ… Find performance bottlenecks during exploitation
```

### ðŸŽ¯ **Full Stack Testing**

```
1ï¸âƒ£ Functional (Automation) â†’ Does it work?
2ï¸âƒ£ Performance â†’ Does it work fast enough?
3ï¸âƒ£ Security â†’ Can it be broken?
```

---

## ðŸ“Š **TEST METRICS & KPIs**

### ðŸ¤– **Automation Metrics**

| Metric | ðŸŽ¯ What It Shows | âœ… Target |
|--------|-------------------|-----------|
| **Test Coverage** | % code tested | > 80% |
| **Pass Rate** | % tests passing | > 95% |
| **Execution Time** | How long tests take | < 10 min |
| **Flakiness Rate** | % flaky tests | < 5% |

### âš¡ **Performance Metrics**

| Metric | ðŸŽ¯ What It Shows | âœ… Target |
|--------|-------------------|-----------|
| **Response Time** | Speed | P95 < 2s |
| **Throughput** | Capacity | High req/s |
| **Error Rate** | Reliability | < 1% |
| **Resource Usage** | Efficiency | CPU < 70% |

### ðŸ” **Security Metrics**

| Metric | ðŸŽ¯ What It Shows | âœ… Target |
|--------|-------------------|-----------|
| **Vulnerabilities Found** | Security issues | Track & fix |
| **Time to Fix** | Remediation speed | < 7 days |
| **Risk Score** | Overall security | Low/Medium |
| **Coverage** | OWASP Top 10 coverage | 100% |

---

## ðŸŽ“ **ISTQB QUICK REFERENCE**

### ðŸ“˜ **Test Automation Engineer Concepts**

```
Test Automation Architecture:
ðŸ”¹ Test Generation â†’ Test Definition â†’ Test Execution â†’ Test Adaptation â†’ Testability

Framework Types:
ðŸ”¹ Linear â†’ Data-Driven â†’ Keyword-Driven â†’ Modular â†’ Hybrid

Design Patterns:
ðŸ”¹ Page Object Model â†’ Factory â†’ Builder â†’ Strategy â†’ Observer

Test Automation Lifecycle:
1ï¸âƒ£ Planning â†’ 2ï¸âƒ£ Design â†’ 3ï¸âƒ£ Implementation â†’ 4ï¸âƒ£ Execution â†’ 5ï¸âƒ£ Maintenance â†’ 6ï¸âƒ£ Retirement
```

### âœ… **ISTQB TAE Principles**

```
âœ… Not all tests should be automated
âœ… Test automation requires maintenance
âœ… Test automation is software development
âœ… Test automation should be treated as a project
âœ… Test automation requires skills and resources
âœ… Test automation should be integrated early
âœ… Test automation ROI improves over time
```

---

## ðŸŽ¯ **OWASP QUICK REFERENCE**

### ðŸ” **Top 10:2021 Summary**

```
A01 ðŸ”´ Broken Access Control
A02 ðŸ”´ Cryptographic Failures  
A03 ðŸ”´ Injection
A04 ðŸŸ¡ Insecure Design
A05 ðŸŸ¡ Security Misconfiguration
A06 ðŸŸ¡ Vulnerable Components
A07 ðŸ”´ Identification & Authentication Failures
A08 ðŸŸ¡ Software/Data Integrity Failures
A09 ðŸŸ¢ Security Logging Failures
A10 ðŸŸ¡ SSRF
```

### ðŸ› ï¸ **OWASP Testing Tools**

| Tool | ðŸŽ¯ Purpose |
|------|------------|
| **OWASP ZAP** | Web app security scanner |
| **Burp Suite** | Web security testing |
| **sqlmap** | SQL injection tool |
| **Nikto** | Web server scanner |
| **Nmap** | Network discovery |

---

## ðŸš€ **QUICK START COMMANDS**

### ðŸ¤– **Automation**

```bash
# Install dependencies
pip install -r requirements.txt
playwright install chromium

# Run tests
pytest
pytest --html=report.html
pytest --cov=.
```

### âš¡ **Performance**

```bash
# Cross-repo performance test
python cross_repo_performance_tester.py

# k6 test
k6 run k6/basic_api_test.js

# Locust test
locust -f locust/locustfile.py --users 50
```

### ðŸ” **Security**

```bash
# Start vulnerable apps
docker-compose up -d

# Run OWASP Top 10 tests
python tools/testing/test_dvwa_owasp_top10.py

# Specific attacks
python tools/attacks/attack_sqli_basic.py
python tools/attacks/attack_xss_basic.py
```

---

## ðŸ“ **TESTING CHECKLISTS**

### âœ… **Before Testing**

```
âœ… Understand requirements
âœ… Set up test environment
âœ… Prepare test data
âœ… Define test objectives
âœ… Set up monitoring
âœ… Backup data
```

### âœ… **During Testing**

```
âœ… Execute test cases
âœ… Monitor system resources
âœ… Log all findings
âœ… Capture screenshots/evidence
âœ… Document anomalies
âœ… Track metrics
```

### âœ… **After Testing**

```
âœ… Analyze results
âœ… Identify root causes
âœ… Create test reports
âœ… Share findings
âœ… Retest fixes
âœ… Update test suites
```

---

## ðŸŽ¯ **COMMON PATTERNS**

### ðŸ”„ **Test Pattern: AAA**

```
Arrange â†’ Set up test data
Act     â†’ Execute action
Assert  â†’ Verify result
```

### ðŸ“Š **Report Pattern**

```
ðŸ“‹ Summary â†’ What was tested
ðŸ“ˆ Results â†’ Pass/Fail counts
ðŸ” Findings â†’ Issues found
ðŸ’¡ Recommendations â†’ What to fix
```

### ðŸŽ¯ **Bug Report Pattern**

```
ðŸ“ Title â†’ Clear description
ðŸ” Steps â†’ How to reproduce
âœ… Expected â†’ What should happen
âŒ Actual â†’ What actually happened
ðŸ“Ž Evidence â†’ Screenshots/logs
```

---

## ðŸ† **SUCCESS CRITERIA**

### ðŸ¤– **Automation Success**

```
âœ… > 80% test coverage
âœ… > 95% pass rate
âœ… < 5% flakiness
âœ… Fast execution (< 10 min)
âœ… Clear reports
```

### âš¡ **Performance Success**

```
âœ… P95 response time < 2s
âœ… Error rate < 1%
âœ… Handles expected load
âœ… No memory leaks
âœ… Scalable architecture
```

### ðŸ” **Security Success**

```
âœ… OWASP Top 10 covered
âœ… Vulnerabilities documented
âœ… Risk assessment complete
âœ… Remediation plan in place
âœ… Regular security scans
```

---

**ðŸ“š Keep this cheat sheet handy! Update as you learn more.** ðŸš€

