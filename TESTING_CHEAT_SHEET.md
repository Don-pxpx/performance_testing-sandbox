# ğŸ¯ Testing Cheat Sheet
**Quick Reference Guide - ISTQB & OWASP Aligned**

---

## ğŸ¤– **AUTOMATION TESTING** (ISTQB Foundation)

### ğŸ“Š **Test Design Techniques**

| Technique | ğŸ¯ What It Is | ğŸ“ Example | âœ… When to Use |
|-----------|---------------|------------|----------------|
| **Equivalence Partitioning** | Group similar inputs | Age: 0-17, 18-65, 66+ | One test per group |
| **Boundary Value Analysis** | Test edge values | Min: 0, Max: 100, Min-1: -1, Max+1: 101 | Form limits, ranges |
| **Decision Table** | All input combos | Login: Valid/Invalid user Ã— Valid/Invalid pass | Complex logic |
| **State Transition** | Test state changes | Login â†’ Logout â†’ Login | Workflows |
| **Use Case Testing** | User scenarios | "As user, I want to checkout" | End-to-end flows |

### ğŸ“ˆ **Test Levels**

```
ğŸ”¹ Unit Testing        â†’ Test individual functions
ğŸ”¹ Integration Testing â†’ Test components together
ğŸ”¹ System Testing      â†’ Test entire system
ğŸ”¹ Acceptance Testing   â†’ Test user requirements
```

### ğŸ¨ **Test Types**

| Type | ğŸ¯ Purpose | ğŸ› ï¸ Tools |
|------|-----------|----------|
| **Functional** | Does it work? | Playwright, Selenium |
| **Non-Functional** | How well does it work? | Performance, Security tools |
| **Structural** | Code coverage | pytest-cov, coverage.py |
| **Change-Related** | Regression | Test suites |

### âœ… **Quick Commands**

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=.

# Run specific test
pytest tests/saucedemo/test_login.py

# Run with HTML report
pytest --html=report.html --self-contained-html
```

---

## âš¡ **PERFORMANCE TESTING** (ISTQB Performance)

### ğŸ“Š **Test Types**

| Type | ğŸ¯ Purpose | ğŸ“ˆ Load | â±ï¸ Duration | ğŸ¯ Goal |
|------|-----------|--------|-------------|---------|
| **Load Testing** | Normal load | Expected users | 30-60 min | Verify performance |
| **Stress Testing** | Beyond capacity | Max + 20% | Until failure | Find breaking point |
| **Spike Testing** | Sudden traffic | 0 â†’ Max â†’ 0 | 5-10 min | Handle spikes |
| **Volume Testing** | Large data | Normal load | 1+ hour | Data handling |
| **Endurance Testing** | Long duration | Normal load | 4+ hours | Memory leaks |

### ğŸ“ˆ **Key Metrics**

| Metric | ğŸ¯ What It Means | âœ… Good | âŒ Bad |
|--------|-------------------|---------|--------|
| **Response Time** | Time to respond | < 2s | > 5s |
| **Throughput** | Requests/second | High | Low |
| **Error Rate** | Failed requests | < 1% | > 5% |
| **CPU Usage** | Processor load | < 70% | > 90% |
| **Memory Usage** | RAM consumption | Stable | Growing |
| **P95/P99** | 95th/99th percentile | < 3s | > 10s |

### ğŸ› ï¸ **Tools Quick Reference**

| Tool | ğŸ¯ Best For | ğŸ“ Command |
|------|-------------|------------|
| **k6** | Modern, scriptable | `k6 run script.js` |
| **Locust** | Python-based | `locust -f locustfile.py` |
| **JMeter** | GUI + scripting | `jmeter -n -t test.jmx` |
| **Gatling** | Scala, reports | `gatling.sh` |

### âœ… **Performance Checklist**

```
âœ… Define performance goals (SLA)
âœ… Create realistic test data
âœ… Monitor resources (CPU, Memory, Network)
âœ… Run baseline test first
âœ… Compare before/after changes
âœ… Document bottlenecks
âœ… Retest after fixes
```

---

## ğŸ” **PENETRATION TESTING** (OWASP Top 10)

### ğŸ¯ **OWASP Top 10:2021**

| # | Vulnerability | ğŸ”´ Risk | ğŸ› ï¸ Test For | âœ… Status |
|---|---------------|---------|-------------|-----------|
| **A01** | Broken Access Control | ğŸ”´ğŸ”´ğŸ”´ High | IDOR, privilege escalation | âœ… Covered |
| **A02** | Cryptographic Failures | ğŸ”´ğŸ”´ğŸ”´ High | Plaintext passwords, weak encryption | âšª Next |
| **A03** | Injection | ğŸ”´ğŸ”´ğŸ”´ High | SQLi, NoSQLi, Command Injection | âœ… Covered |
| **A04** | Insecure Design | ğŸ”´ğŸ”´ Medium | Business logic flaws | âšª Next |
| **A05** | Security Misconfiguration | ğŸ”´ğŸ”´ Medium | Default configs, exposed files | âšª Next |
| **A06** | Vulnerable Components | ğŸ”´ğŸ”´ Medium | Outdated libraries, CVEs | âšª Next |
| **A07** | Auth Failures | ğŸ”´ğŸ”´ğŸ”´ High | Weak passwords, session issues | âœ… Covered |
| **A08** | Data Integrity Failures | ğŸ”´ğŸ”´ Medium | Unsigned updates, CI/CD attacks | âšª Next |
| **A09** | Logging Failures | ğŸ”´ Low | Missing logs, log injection | âšª Next |
| **A10** | SSRF | ğŸ”´ğŸ”´ Medium | Internal network access | âšª Next |

### ğŸ” **Common Attack Vectors**

| Attack | ğŸ¯ Target | ğŸ’¥ Impact | ğŸ› ï¸ Tool |
|--------|-----------|-----------|---------|
| **SQL Injection** | Database | Data breach, auth bypass | sqlmap, manual |
| **XSS** | Users | Cookie theft, defacement | Burp Suite, manual |
| **IDOR** | API endpoints | Unauthorized access | Manual testing |
| **CSRF** | State-changing ops | Unauthorized actions | Burp Suite |
| **SSRF** | Internal services | Internal network access | Manual testing |
| **XXE** | XML parsers | File read, SSRF | Manual testing |

### ğŸ¯ **Testing Methodology**

```
1ï¸âƒ£ Reconnaissance    â†’ ğŸ” Info gathering
2ï¸âƒ£ Scanning          â†’ ğŸ¯ Find vulnerabilities  
3ï¸âƒ£ Exploitation      â†’ ğŸ’¥ Prove impact
4ï¸âƒ£ Post-Exploitation â†’ ğŸ“Š Maintain access
5ï¸âƒ£ Reporting        â†’ ğŸ“ Document findings
```

### âœ… **Quick Commands**

```bash
# SQL Injection test
python tools/attacks/attack_sqli_basic.py

# XSS test
python tools/attacks/attack_xss_basic.py

# API endpoint discovery
python tools/discovery/discovery_api_endpoints.py

# OWASP Top 10 comprehensive test
python tools/testing/test_dvwa_owasp_top10.py
```

---

## ğŸ”— **CROSS-REPO INTEGRATION**

### ğŸ¤–âš¡ **Automation + Performance**

```
âœ… Add performance assertions to UI tests
âœ… Fail test if page load > 3s
âœ… Monitor API response times in automation
âœ… Track performance trends over time
```

### ğŸ¤–ğŸ” **Automation + Security**

```
âœ… Test for XSS in form submissions
âœ… Check for exposed secrets in responses
âœ… Validate authentication flows
âœ… Test authorization boundaries
```

### âš¡ğŸ” **Performance + Security**

```
âœ… Run attacks under load (hybrid testing)
âœ… Test if system fails faster under stress + attack
âœ… Monitor resource usage during attacks
âœ… Find performance bottlenecks during exploitation
```

### ğŸ¯ **Full Stack Testing**

```
1ï¸âƒ£ Functional (Automation) â†’ Does it work?
2ï¸âƒ£ Performance â†’ Does it work fast enough?
3ï¸âƒ£ Security â†’ Can it be broken?
```

---

## ğŸ“Š **TEST METRICS & KPIs**

### ğŸ¤– **Automation Metrics**

| Metric | ğŸ¯ What It Shows | âœ… Target |
|--------|-------------------|-----------|
| **Test Coverage** | % code tested | > 80% |
| **Pass Rate** | % tests passing | > 95% |
| **Execution Time** | How long tests take | < 10 min |
| **Flakiness Rate** | % flaky tests | < 5% |

### âš¡ **Performance Metrics**

| Metric | ğŸ¯ What It Shows | âœ… Target |
|--------|-------------------|-----------|
| **Response Time** | Speed | P95 < 2s |
| **Throughput** | Capacity | High req/s |
| **Error Rate** | Reliability | < 1% |
| **Resource Usage** | Efficiency | CPU < 70% |

### ğŸ” **Security Metrics**

| Metric | ğŸ¯ What It Shows | âœ… Target |
|--------|-------------------|-----------|
| **Vulnerabilities Found** | Security issues | Track & fix |
| **Time to Fix** | Remediation speed | < 7 days |
| **Risk Score** | Overall security | Low/Medium |
| **Coverage** | OWASP Top 10 coverage | 100% |

---

## ğŸ“ **ISTQB QUICK REFERENCE**

### ğŸ“˜ **Foundation Level Concepts**

```
Test Levels:
ğŸ”¹ Unit â†’ Integration â†’ System â†’ Acceptance

Test Types:
ğŸ”¹ Functional â†’ Non-Functional â†’ Structural â†’ Change-Related

Test Design:
ğŸ”¹ Equivalence Partitioning
ğŸ”¹ Boundary Value Analysis
ğŸ”¹ Decision Tables
ğŸ”¹ State Transition
ğŸ”¹ Use Case Testing

Test Process:
1ï¸âƒ£ Planning â†’ 2ï¸âƒ£ Design â†’ 3ï¸âƒ£ Implementation â†’ 4ï¸âƒ£ Execution â†’ 5ï¸âƒ£ Reporting
```

### âœ… **ISTQB Principles**

```
âœ… Testing shows presence of defects
âœ… Exhaustive testing is impossible
âœ… Early testing saves time & money
âœ… Defects cluster together
âœ… Tests wear out (need updates)
âœ… Testing is context dependent
âœ… Absence-of-errors fallacy
```

---

## ğŸ¯ **OWASP QUICK REFERENCE**

### ğŸ” **Top 10:2021 Summary**

```
A01 ğŸ”´ Broken Access Control
A02 ğŸ”´ Cryptographic Failures  
A03 ğŸ”´ Injection
A04 ğŸŸ¡ Insecure Design
A05 ğŸŸ¡ Security Misconfiguration
A06 ğŸŸ¡ Vulnerable Components
A07 ğŸ”´ Identification & Authentication Failures
A08 ğŸŸ¡ Software/Data Integrity Failures
A09 ğŸŸ¢ Security Logging Failures
A10 ğŸŸ¡ SSRF
```

### ğŸ› ï¸ **OWASP Testing Tools**

| Tool | ğŸ¯ Purpose |
|------|------------|
| **OWASP ZAP** | Web app security scanner |
| **Burp Suite** | Web security testing |
| **sqlmap** | SQL injection tool |
| **Nikto** | Web server scanner |
| **Nmap** | Network discovery |

---

## ğŸš€ **QUICK START COMMANDS**

### ğŸ¤– **Automation**

```bash
# Install dependencies
pip install -r requirements.txt
playwright install chromium

# Run tests
pytest
pytest --html=report.html
pytest --cov=.
```

### âš¡ **Performance**

```bash
# Cross-repo performance test
python cross_repo_performance_tester.py

# k6 test
k6 run k6/basic_api_test.js

# Locust test
locust -f locust/locustfile.py --users 50
```

### ğŸ” **Security**

```bash
# Start vulnerable apps
docker-compose up -d

# Run OWASP Top 10 tests
python tools/testing/test_dvwa_owasp_top10.py

# Specific attacks
python tools/attacks/attack_sqli_basic.py
python tools/attacks/attack_xss_basic.py
```

---

## ğŸ“ **TESTING CHECKLISTS**

### âœ… **Before Testing**

```
âœ… Understand requirements
âœ… Set up test environment
âœ… Prepare test data
âœ… Define test objectives
âœ… Set up monitoring
âœ… Backup data
```

### âœ… **During Testing**

```
âœ… Execute test cases
âœ… Monitor system resources
âœ… Log all findings
âœ… Capture screenshots/evidence
âœ… Document anomalies
âœ… Track metrics
```

### âœ… **After Testing**

```
âœ… Analyze results
âœ… Identify root causes
âœ… Create test reports
âœ… Share findings
âœ… Retest fixes
âœ… Update test suites
```

---

## ğŸ¯ **COMMON PATTERNS**

### ğŸ”„ **Test Pattern: AAA**

```
Arrange â†’ Set up test data
Act     â†’ Execute action
Assert  â†’ Verify result
```

### ğŸ“Š **Report Pattern**

```
ğŸ“‹ Summary â†’ What was tested
ğŸ“ˆ Results â†’ Pass/Fail counts
ğŸ” Findings â†’ Issues found
ğŸ’¡ Recommendations â†’ What to fix
```

### ğŸ¯ **Bug Report Pattern**

```
ğŸ“ Title â†’ Clear description
ğŸ” Steps â†’ How to reproduce
âœ… Expected â†’ What should happen
âŒ Actual â†’ What actually happened
ğŸ“ Evidence â†’ Screenshots/logs
```

---

## ğŸ† **SUCCESS CRITERIA**

### ğŸ¤– **Automation Success**

```
âœ… > 80% test coverage
âœ… > 95% pass rate
âœ… < 5% flakiness
âœ… Fast execution (< 10 min)
âœ… Clear reports
```

### âš¡ **Performance Success**

```
âœ… P95 response time < 2s
âœ… Error rate < 1%
âœ… Handles expected load
âœ… No memory leaks
âœ… Scalable architecture
```

### ğŸ” **Security Success**

```
âœ… OWASP Top 10 covered
âœ… Vulnerabilities documented
âœ… Risk assessment complete
âœ… Remediation plan in place
âœ… Regular security scans
```

---

**ğŸ“š Keep this cheat sheet handy! Update as you learn more.** ğŸš€

